<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="CSS/index3.css"/>
    <link rel="shortcut icon" href="Images/favicon.png" type="image/x-icon">
    <title>Kashyap Holla Blog 3</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL" crossorigin="anonymous"></script>
</head>
<body>
    <div class="container">
            <h1>Building a Naive Bayes Classifier to Identify AI-Generated Text</h1>
            <p>This blog post deals with the task of distinguishing between human and machine-generated text with the implementation of a Naive Bayes Classifier (NBC) to solve this problem. We have built a NBC model that tokenizes the dataset, builds a vocabulary, calculates probabilities, and utilizes these probabilities to predict whether an essay is generated by a human or a Large Language Model (LLM).</p>
            <h3>1. Setting up the environment</h3>
            <p>We start by importing necessary libraries including NLTK. NLTK comes with various tools for performing tasks such as tokenization, lemmatization, and removing stopwords and punctuation. The tokenize_essay function processes the text data from our essays. It converts text to lowercase, lemmatizes it, and removes stopwords and punctuation.</p>
            <h3>2. Data Loading and Preprocessing</h3>
            <p>Using pandas, we load our dataset and apply the tokenization function. We use CountVectorizer from sklearn to convert our tokenized essays into a matrix of token counts and build our vocabulary.
            </p>
            <h3>3. Filtering Rare Words, Building a Reverse Index and Calculating the probabilities</h3>
            <p>We have written a code logic that filters out words that occur less than five times and creates a reverse index that maps words to their indices in our feature matrix. We compute the probability of each word's occurrence and the conditional probabilities based on the classification (human or LLM).</p>
            <h3>4. Model Building, Training and Evaluation</h3>
            <p>We define the BayesianTextClassifier class with methods for fitting the model, predicting probabilities, and predicting class labels. With our classifier defined, we train it on our dataset and evaluate its performance using a classification report.</p>
            <h3>5. Hyperparameter Tuning and Identifying Key Predictive Words</h3>
            <p>We iterate over various alpha values to tune the Laplace smoothing parameter. We extract the top words that are most indicative of each class and visualize the top predictive words using bar charts.
            </p>
            <h3>6. Reference</h3>
            <p>The code used in this text classification task was referenced from: <br> <a href="https://medium.com/@keerthichowdary400/naive-bayes-classification-detect-ai-generated-text-5b7cf48b251b#:~:text=LLM%20%E2%80%94%20Detect%20AI%2DGenerated%20Text,generated%20and%20AI%2Dgenerated%20text." target="_blank">https://medium.com/@keerthichowdary400/naive-bayes-classification-detect-ai-generated-text-5b7cf48b251b#:~:text=LLM%20%E2%80%94%20Detect%20AI%2DGenerated%20Text,generated%20and%20AI%2Dgenerated%20text.</a></p>
            <h3>7. My Contribution</h3>
            <ul>
                <li><u><i>Generating Additional Training Data with ChatGPT</i></u></li>
                <p>Recognizing the dataset's bias towards human-written essays, I used ChatGPT to generate additional essays. These were then merged with the original dataset. This was used when training the model on my local device. The code does not contain this file as the editor used for building the model was Kaggle's in-built editor which did not allow importing files and using it for the model.</p>
                <li><u><i>Building a Filtered Vocabulary List</i></u></li>
                <p>I developed a method to create a vocabulary list that excludes rare words (those occurring less than five times), reducing noise and focusing the model on more impactful words.</p>
                <li><u><i>Creating a Reverse Index</i></u></li>
                <p>I implemented a reverse index for the filtered vocabulary, mapping each word to its index in the feature matrix, which streamlined subsequent computations.</p>
                <li><u><i>Calculating Word Occurrence Probabilities</i></u></li>
                <p>I introduced an approach to calculate the probability of each word's occurrence in the filtered vocabulary, providing a foundational understanding of word importance across all documents.</p>
                <li><u><i>Determining Conditional Probabilities</i></u></li>
                <p>My approach involved calculating the likelihood of each word appearing in human-written and LLM-generated essays, which are crucial for the Naive Bayes classification.</p>
                <li><u><i>Visualization with Matplotlib</i></u></li>
                <p>I employed Matplotlib to visualize the top predictive words, making the model's insights more accessible and interpretable.</p>
            </ul>
            <p>These contributions led to a significant improvement in the model's performance, resulting in a 10% increase in accuracy, demonstrating the effectiveness of these enhancements in distinguishing between human and AI-generated text.<br><a class="btn-header-main-page" href="https://www.kaggle.com/kashyapholla" target="_blank">My Kaggle Page</a></p>
            <h3>8. Code</h3>
            <a class="btn-header-main-page" href="Data/text_classifier.ipynb" download="Data/text_classifier.ipynb" target="_blank">Kashyap_Holla_Text_Classifier.ipynb</a>
            <!-- <a href="Data/titanic.ipynb" download="Data/titanic.ipynb" target="_blank">Code</a> -->
            <img src="TextClassifier_After.jpg" alt="Submission" width="100%">
    </div>   
</body>
</html>
